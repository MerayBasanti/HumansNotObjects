# HumansNotObjects
A deep learning image classifier that distinguishes human faces from non-human objects using MobileNetV2, with adversarial robustness testing (FGSM).

# Human vs Object Image Classifier ðŸš€

A deep learning image classifier that distinguishes **human faces** (from the LFW dataset) from **non-human objects** (from an additional dataset).  
It uses **Transfer Learning** with **MobileNetV2** and includes **adversarial robustness testing** with the **Fast Gradient Sign Method (FGSM)**.

---

## ðŸ“Œ **Project Highlights**

âœ… Uses **Labeled Faces in the Wild (LFW)** for human face images.  
âœ… Uses an additional objects dataset for non-human images.  
âœ… Fine-tunes **MobileNetV2** for binary classification: *Is it human or not?*  
âœ… Includes clear **train**, **validation**, and **test** splits.  
âœ… Tests the modelâ€™s robustness with **FGSM adversarial attacks**.  
âœ… Provides defense suggestions: adversarial training, preprocessing, distillation.

---

## ðŸ—‚ï¸ **Project Structure**

```plaintext
ðŸ“‚ lfw-deepfunneled/
   â””â”€â”€ [folders of person images]

ðŸ“‚ objects/
   â””â”€â”€ [folders or images of objects]

ðŸ“„ notebook.ipynb      # Main Colab / Jupyter Notebook
ðŸ“„ my_mobilenetv2_classifier.keras  # Saved trained model
ðŸ“„ README.md           # This file
```

---

## âš™ï¸ **How to Run**

### âœ… 1ï¸âƒ£ Prepare Datasets

* Place the **LFW** dataset under `lfw-deepfunneled/` (organized in folders by person).
* Place your **object images** in a separate folder.

### âœ… 2ï¸âƒ£ Train the Model

* Use the notebook to:

  * Load data with `pandas` + `ImageDataGenerator`.
  * Create `train`, `validation`, and `test` splits.
  * Fine-tune **MobileNetV2** on your binary labels.
  * Save your trained model:

    ```python
    model.save('my_mobilenetv2_classifier.keras')
    ```

### âœ… 3ï¸âƒ£ Test Adversarial Robustness

* Load the saved model:

  ```python
  from tensorflow.keras.models import load_model
  model = load_model('my_mobilenetv2_classifier.keras')
  ```

* Run the **FGSM loop** to generate adversarial examples and measure the impact.

---

## âœ… **Final Adversarial Results**

After testing with **FGSM attack** (`epsilon = 0.01`):

* **Total test samples:** `6485`
* **Clean accuracy:** `100%`
* **Adversarial accuracy:** `91.13%`

**Adversarial confusion matrix:**

|                   | Predicted Object | Predicted Human |
| ----------------- | ---------------- | --------------- |
| **Actual Object** | 4436             | 64              |
| **Actual Human**  | 511              | 1474            |

**Interpretation:**

* The model achieves perfect accuracy on clean test images.
* Under FGSM, the accuracy drops to ~91%, with human faces more affected than objects.
* About **511 human faces** were misclassified as objects under adversarial noise â€” a common vulnerability for gradient-based attacks.

---

## ðŸ” **Adversarial Robustness Tips**

âœ… **Adversarial training**: Fine-tune your model on a mix of clean + adversarial examples for better resilience.

âœ… **Input preprocessing defenses**: Add denoising, compression, or random noise layers to disrupt gradient-based attacks.

âœ… **Stronger attack testing**: Evaluate with **PGD**, **BIM**, or **C&W** for deeper robustness insights.

---

## ðŸ§© **Dependencies**

* `TensorFlow >= 2.x`
* `scikit-learn`
* `pandas`
* `numpy`

Install with:

```bash
pip install tensorflow scikit-learn pandas numpy
```

---

## ðŸš€ **Next Steps**

* Add **adversarial training** to improve robustness.
* Experiment with stronger attacks and defense strategies.
* Deploy the model with **TensorFlow Lite**, **ONNX**, or an edge device.

---

## âœ¨ **Credits**

* **LFW Dataset** â€” University of Massachusetts Amherst.
* **MobileNetV2** â€” Pretrained on ImageNet.

---

## ðŸ“¬ **Contact**

Questions or suggestions?
Feel free to open an issue or pull request!
Happy experimenting â€” keep your models robust! ðŸŽ‰
